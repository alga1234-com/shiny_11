axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",            # Hides legend (optional)
plot.title = element_text(face = "bold", size = 14)
) +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
)
library(ggplot2)
library(ggplot2)
# Custom theme
theme_custom <- theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 16, color = "#003366"),
axis.title = element_text(face = "bold", size = 13, color = "#333333"),
axis.text = element_text(size = 11, color = "#333333"),
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid.major = element_line(color = "#dddddd"),
panel.grid.minor = element_blank()
)
# Plot using custom theme
ggplot(combined_data, aes(x = Neighborhood, y = Sale_Price, fill = Neighborhood)) +
geom_boxplot() +
scale_fill_brewer(palette = "Blues") +
theme_custom +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
) +
guides(fill = "none")
custom_colors <- c(
"#003366", "#336699", "#6699CC", "#99CCFF", "#339999"
)
# Use in a plot
ggplot(combined_data, aes(x = Neighborhood, y = Sale_Price, fill = Neighborhood)) +
geom_boxplot() +
scale_fill_manual(values = custom_colors) +
theme_minimal() +
theme(
plot.title = element_text(color = "#003366", size = 16, face = "bold"),
axis.title = element_text(color = "#4D4D4D", face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
panel.grid.major = element_line(color = "#E0E0E0")
) +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
)
library(ggplot2)
library(ggplot2)
custom_colors <- c(
"#003366", "#336699", "#6699CC", "#99CCFF", "#339999"
)
# Use in a plot
ggplot(combined_data, aes(x = Neighborhood, y = Sale_Price, fill = Neighborhood)) +
geom_boxplot() +
scale_fill_manual(values = custom_colors) +
theme_minimal() +
theme(
plot.title = element_text(color = "#003366", size = 16, face = "bold"),
axis.title = element_text(color = "#4D4D4D", face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
panel.grid.major = element_line(color = "#E0E0E0")
) +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
)
custom_colors <- c(
"#003366", "#336699", "#6699CC", "#99CCFF", "#339999"
)
ggplot(combined_data, aes(x = Neighborhood, y = Sale_Price, fill = Neighborhood)) +
geom_boxplot() +
scale_fill_manual(values = custom_colors) +
theme_minimal() +
theme(
plot.title = element_text(color = "#003366", size = 16, face = "bold"),
axis.title = element_text(color = "#4D4D4D", face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
panel.grid.major = element_line(color = "#E0E0E0")
) +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
)
ggplot(combined_data, aes(x = Neighborhood, y = Sale_Price, fill = Neighborhood)) +
geom_boxplot() +
scale_fill_brewer(palette = "Set3") +  # Handles many categories
theme_minimal() +
theme(
plot.title = element_text(color = "#003366", size = 16, face = "bold"),
axis.title = element_text(color = "#4D4D4D", face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
panel.grid.major = element_line(color = "#E0E0E0")
) +
labs(
title = "Sale Price by Neighborhood",
x = "Neighborhood",
y = "Sale Price"
)
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
vif_values <- vif(lm_model)
print(vif_values)
install.packages("car")
library(car)
print(vif_values)
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
install.packages("car")
library(car)
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
install.packages("car")
library(car)
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
library(car)
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
library(dplyr)
library(tidyverse)  # loads dplyr and others
lm_model <- lm(Sale_Price ~ ., data = combined_data %>% select(Sale_Price, all_of(names(predictors))))
vif_values <- vif(lm_model)
print(vif_values)
set.seed(123)
split <- initial_split(combined_data, prop = 0.8)
library(rsample)
set.seed(123)
split <- initial_split(combined_data, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
train_data <- training(split)
test_data <- testing(split)
model <- lm(SalePrice ~ ., data = train_data)
model <- lm(Sale_Price ~ ., data = train_data)
train_matrix <- model.matrix(SalePrice ~ . - 1, data = train_da
library(xgboost)
train_matrix <- model.matrix(Sale_Price ~ . - 1, data = train_data)
library(xgboost)
model <- xgboost(data = train_matrix,
label = train_data$SalePrice,
nrounds = 100,
objective = "reg:squarederror")
model <- xgboost(data = train_matrix,
label = train_data$Sale_Price,
nrounds = 100,
objective = "reg:squarederror")
test_matrix <- model.matrix(SalePrice ~ . - 1, data = test_data)
test_matrix <- model.matrix(~ . - 1, data = test_data)
predictions <- predict(model, newdata = test_matrix)
train_matrix <- model.matrix(Sale_Price ~ . - 1, data = train_data)
train_label <- train_data$Sale_Price
model <- xgboost(data = train_matrix,
label = train_data$Sale_Price,
nrounds = 100,
objective = "reg:squarederror")
test_matrix <- model.matrix(~ . - 1, data = test_data)
predictions <- predict(model, newdata = test_matrix)
library(dplyr)
library(tidyverse)
library(car)
library(rsample)
library(xgboost)
library(ggplot2)
library(reshape2)
library(corrplot)
combined_data <- combined_data %>% rename(SalePrice = Sale_Price)
predictors <- combined_data %>%
select(-SalePrice) %>%
select(where(is.numeric)) %>%
names()
lm_model <- lm(SalePrice ~ ., data = combined_data %>% select(SalePrice, all_of(predictors)))
vif_values <- vif(lm_model)
print(vif_values)
set.seed(123)
split <- initial_split(combined_data, prop = 0.8)
train_data <- training(split)
test_data <- testing(split)
test_data$SalePrice <- NA
full_data <- rbind(train_data, test_data)
full_matrix <- model.matrix(SalePrice ~ . - 1, data = full_data)
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
test_data$SalePrice <- NA
full_data <- rbind(train_data, test_data)
common_cols <- intersect(names(train_data), names(test_data))
if (!"SalePrice" %in% names(test_data)) {
test_data$SalePrice <- NA
}
train_data <- train_data[, common_cols]
test_data  <- test_data[, common_cols]
full_data <- rbind(train_data, test_data)
train_label <- train_data$SalePrice
xgb_model <- xgboost(
data = train_matrix,
label = train_label,
nrounds = 100,
objective = "reg:squarederror",
verbose = 0
)
predictions <- predict(xgb_model, newdata = test_matrix)
full_matrix <- model.matrix(SalePrice ~ . - 1, data = full_data)
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
nrow(train_data)
nrow(test_data)
nrow(full_data)
full_matrix <- model.matrix(~ . - 1, data = full_data)
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
nrow(full_matrix)
full_matrix <- model.matrix(~ . - 1, data = full_data)
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
n_train <- nrow(train_data)
n_full <- nrow(full_matrix)
print(c(n_train, n_full))
full_matrix <- model.matrix(~ . - 1, data = full_data)
full_data <- rbind(train_data, test_data)
full_matrix <- model.matrix(~ . - 1, data = full_data)
print(c(nrow(train_data), nrow(test_data), nrow(full_data), nrow(full_matrix)))
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
n_train <- nrow(train_data)
n_full <- nrow(full_matrix)
cat("Rows train_data:", n_train, "\nRows full_matrix:", n_full, "\n")
n_test <- nrow(test_data)
n_full_data <- nrow(full_data)
cat("Rows test_data:", n_test, "\nRows full_data:", n_full_data, "\n")
n_full <- nrow(full_matrix)
cat("Rows full_matrix after fix:", n_full, "\n")
n_test <- nrow(test_data)
n_full_data <- nrow(full_data)
cat("Rows test_data:", n_test, "\nRows full_data:", n_full_data, "\n")
full_matrix <- model.matrix(~ . - 1, data = full_data)
n_full <- nrow(full_matrix)
cat("Rows full_matrix after fix:", n_full, "\n")
print(nrow(full_data))   # You said 5860 before; confirm again now
test_rows <- full_data[(nrow(train_data) + 1):nrow(full_data), ]
print(paste("Test rows in full_data:", nrow(test_rows)))
test_matrix_check <- model.matrix(~ . - 1, data = test_rows)
library(dplyr)
test_rows %>%
select(where(is.factor)) %>%
summarise(across(everything(), ~ nlevels(.))) %>%
pivot_longer(everything(), names_to = "variable", values_to = "levels") %>%
filter(levels < 2)
factor_levels <- sapply(test_rows, function(col) if(is.factor(col)) nlevels(col) else NA)
print(factor_levels[factor_levels < 2 & !is.na(factor_levels)])
na_counts <- sapply(test_rows, function(col) sum(is.na(col)))
total_rows <- nrow(test_rows)
na_percentage <- na_counts / total_rows * 100
print(na_percentage[na_percentage > 90])  # Show columns with >90% missing
full_matrix <- model.matrix(~ . - 1, data = full_data %>% select(-SalePrice))
train_matrix <- full_matrix[1:nrow(train_data), ]
test_matrix  <- full_matrix[(nrow(train_data) + 1):nrow(full_matrix), ]
library(dplyr)
full_matrix <- model.matrix(~ . - 1, data = full_data %>% select(-SalePrice))
cat("Rows in full_matrix:", nrow(full_matrix), "\n")  # Should be 5860
n_train <- nrow(train_data)
train_matrix <- full_matrix[1:n_train, ]
test_matrix  <- full_matrix[(n_train + 1):nrow(full_matrix), ]
cat("Train matrix rows:", nrow(train_matrix), "\n")  # Should be same as train_data
cat("Test matrix rows:", nrow(test_matrix), "\n")    # Should be same as test_data
set.seed(123)
split <- initial_split(combined_data, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
full_data <- bind_rows(train_data, test_data)
full_matrix <- model.matrix(~ . - 1, data = full_data %>% select(-SalePrice))
n_train <- nrow(train_data)
train_matrix <- full_matrix[1:n_train, ]
test_matrix  <- full_matrix[(n_train + 1):nrow(full_matrix), ]
train_label <- train_data$SalePrice
model <- xgboost(
data = train_matrix,
label = train_label,
nrounds = 100,
objective = "reg:squarederror"
)
predictions <- predict(model, test_matrix)
if ("SalePrice" %in% colnames(test_data) && !all(is.na(test_data$SalePrice))) {
actual <- test_data$SalePrice
rmse <- sqrt(mean((predictions - actual)^2))
print(paste("Test RMSE:", rmse))
} else {
print("Test SalePrice not available; cannot compute RMSE.")
}
results <- data.frame(ID = 1:nrow(test_data),
Actual = test_data$SalePrice,
Predicted = predictions)
write.csv(results, "model_predictions.csv", row.names = FALSE)
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
xgb.save(xgb_model, paste0("xgboost_model_", timestamp, ".model"))
saveRDS(xgb_model, "learning_model.rds")
save.image("learning_model.RData")
data <- read.csv("model_predictions.csv")
numeric_data <- data[sapply(data, is.numeric)]
cor_matrix <- cor(numeric_data, use = "complete.obs")
melted_cor <- melt(cor_matrix)
ggplot(melted_cor, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) +
coord_fixed()
# Optional prettier version
corrplot(cor_matrix, method = "color", type = "upper",
tl.col = "black", tl.srt = 45, addCoef.col = "black")
write.csv(results_df, "model_predictions.csv", row.names = FALSE)
write.csv(results_df, "model_predictions22.csv", row.names = FALSE)
results_df <- data.frame(test_matrix, Predicted = predictions, Actual = actual)
write.csv(results_df, "model_predictions22.csv", row.names = FALSE)
ggplot(melted_cor, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) +
coord_fixed()
corrplot(cor_matrix, method = "color", type = "upper",
tl.col = "black", tl.srt = 45, addCoef.col = "black")
data <- read.csv("model_predictions.csv")
numeric_data <- data[sapply(data, is.numeric)]
cor_matrix <- cor(numeric_data, use = "complete.obs")
melted_cor <- melt(cor_matrix)
write.csv(importance, "feature_importance.csv", row.names = FALSE)
write.csv(data.frame(ID = 1:nrow(test_data),
Actual = actual,
Predicted = predictions),
"model_predictions.csv", row.names = FALSE)
write.csv(importance, "feature_importance.csv", row.names = FALSE)
feature_names <- colnames(train_matrix)
importance <- xgb.importance(feature_names = feature_names, model = model)
importance <- xgb.importance(model = model)
feature_names <- colnames(train_matrix)
importance <- xgb.importance(feature_names = feature_names, model = model)
head(importance, 10)  # Show top 10 most important features
xgb.plot.importance(importance, top_n = 10)
write.csv(importance, "feature_importance.csv", row.names = FALSE)
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
write.csv(importance, paste0("feature_importance_", timestamp, ".csv"), row.names = FALSE)
rmse <- sqrt(mean((predicted - actual)^2))
actual <- test_data$SalePrice
predicted <- predictions
predictions <- predict(model, test_matrix)
rmse <- sqrt(mean((predicted - actual)^2))
cat("RMSE:", rmse, "\n")
actual <- test_data$SalePrice
predicted <- predict(model, test_matrix)
sst <- sum((actual - mean(actual))^2)        # Total Sum of Squares
sse <- sum((actual - predicted)^2)           # Sum of Squared Errors
rsq <- 1 - sse/sst
cat("R-squared:", rsq, "\n")
full_matrix <- model.matrix(~ . - 1, data = predictors)
n_train <- nrow(train_data)  # from your earlier split
train_matrix <- full_matrix[1:n_train, ]
test_matrix <- full_matrix[(n_train + 1):nrow(full_matrix), ]
full_matrix <- model.matrix(~ . - 1, data = predictors)
class(predictors)
predictors <- full_data_eng %>% select(-SalePrice)
library(dplyr)
full_data_eng <- full_data %>%
mutate(
LogGrLivArea = log(GrLivArea + 1),
OverallQual_Sq = OverallQual^2,
GarageAge = YearSold - GarageYrBlt,
Qual_Area_Interaction = OverallQual * GrLivArea
) %>%
mutate(across(where(is.character), as.factor))
colnames(full_data)
library(dplyr)
full_data_eng <- full_data %>%
mutate(
LogLivArea = log(Liv_Area + 1),
TotalBedrooms_Sq = total_bedrooms^2,
# Example interaction: bedrooms * living area
BedArea_Interaction = total_bedrooms * Liv_Area
) %>%
mutate(across(where(is.character), as.factor))  # convert character cols to factors
predictors <- full_data_eng %>% select(-SalePrice)
full_matrix <- model.matrix(~ . - 1, data = predictors)
set.seed(123)  # for reproducibility
split <- initial_split(full_data_eng, prop = 0.8)
train_data <- training(split)
test_data <- testing(split)
train_predictors <- train_data %>% select(-SalePrice)
test_predictors <- test_data %>% select(-SalePrice)
train_label <- train_data$SalePrice
test_label <- test_data$SalePrice
train_matrix <- model.matrix(~ . - 1, data = train_predictors)
test_matrix <- model.matrix(~ . - 1, data = test_predictors)
model <- xgboost(
data = train_matrix,
label = train_label,
nrounds = 100,
objective = "reg:squarederror",
verbose = 0
)
predictions <- predict(model, newdata = test_matrix)
rmse <- sqrt(mean((predictions - test_label)^2))
sst <- sum((test_label - mean(test_label))^2)
sse <- sum((test_label - predictions)^2)
rsq <- 1 - sse/sst
cat("RMSE:", round(rmse, 2), "\n")
cat("R-squared:", round(rsq, 4), "\n")
library(xgboost)
library(caret)
tune_grid <- expand.grid(
nrounds = c(50, 100, 150),
max_depth = c(3, 6, 9),
eta = c(0.01, 0.1, 0.3),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
train_control <- trainControl(method = "cv", number = 5)
set.seed(123)
xgb_tune <- train(
x = train_matrix,
y = train_label,
trControl = train_control,
tuneGrid = tune_grid,
method = "xgbTree",
verbose = FALSE
)
print(xgb_tune)
best_params <- xgb_tune$bestTune
final_model <- xgboost(
data = train_matrix,
label = train_label,
nrounds = best_params$nrounds,
max_depth = best_params$max_depth,
eta = best_params$eta,
objective = "reg:squarederror",
verbose = 0
)
importance_matrix <- xgb.importance(model = final_model)
xgb.plot.importance(importance_matrix, top_n = 10)
print(head(importance_matrix, 10))
results <- data.frame(
Actual = test_label,
Predicted = final_predictions
)
final_predictions <- predict(final_model, newdata = test_matrix)
results <- data.frame(
Actual = test_label,
Predicted = final_predictions
)
write.csv(results, paste0("model_predictions_", timestamp, ".csv"), row.names = FALSE)
cat("Predictions saved.\n")
library(mlr3)
library(paradox)
search_space = ParamSet$new(list(
ParamDbl$new("cp", lower = 0.001, upper = 0.1),
ParamInt$new("minsplit", lower = 1, upper = 20)
))
model <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
print(xgb_tune)
best_params <- xgb_tune$bestTune
library(tidymodels)
install.packages("tidymodels")
library(tidymodels)
model <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
workflow <- workflow() %>%
add_model(model) %>%
add_formula(target ~ .)
# Define tuning grid
param_set <- parameters(model)
# Use Bayesian optimization
set.seed(123)
results <- tune_bayes(
workflow,
resamples = vfold_cv(training_data, v = 5),
param_info = param_set,
initial = 5,
iter = 20,
metrics = metric_set(accuracy),
control = control_bayes(verbose = TRUE)
)
final_model <- xgboost(
data = train_matrix,
label = train_label,
nrounds = best_params$nrounds,
max_depth = best_params$max_depth,
eta = best_params$eta,
objective = "reg:squarederror",
verbose = 0
)
# Get importance matrix
importance_matrix <- xgb.importance(model = final_model)
xgb.plot.importance(importance_matrix, top_n = 10)
plot(xgb_tune)
print(xgb_tune)
q()
learning_model <- readRDS("C:/Users/kaggwa/Documents/r-project2025/class-project/learning_model.rds")
